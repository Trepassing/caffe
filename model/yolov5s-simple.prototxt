layer {
  name: "images"
  type: "Input"
  top: "images"
  input_param {
    shape {
      dim: 1
      dim: 3
      dim: 640
      dim: 640
    }
  }
}
layer {
  name: "Conv_0"
  type: "Convolution"
  bottom: "images"
  top: "167"
  convolution_param {
    num_output: 32
    bias_term: true
    group: 1
    pad_h: 1
    pad_w: 1
    kernel_h: 3
    kernel_w: 3
    stride_h: 2
    stride_w: 2
    dilation: 1
  }
}
layer {
  name: "LeakyRelu_1"
  type: "ReLU"
  bottom: "167"
  top: "168"
  relu_param {
    negative_slope: 0.10000000149011612
  }
}
layer {
  name: "Conv_2"
  type: "Convolution"
  bottom: "168"
  top: "169"
  convolution_param {
    num_output: 64
    bias_term: true
    group: 1
    pad_h: 1
    pad_w: 1
    kernel_h: 3
    kernel_w: 3
    stride_h: 2
    stride_w: 2
    dilation: 1
  }
}
layer {
  name: "LeakyRelu_3"
  type: "ReLU"
  bottom: "169"
  top: "170"
  relu_param {
    negative_slope: 0.10000000149011612
  }
}
layer {
  name: "Conv_4"
  type: "Convolution"
  bottom: "170"
  top: "171"
  convolution_param {
    num_output: 32
    bias_term: true
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "LeakyRelu_5"
  type: "ReLU"
  bottom: "171"
  top: "172"
  relu_param {
    negative_slope: 0.10000000149011612
  }
}
layer {
  name: "Conv_6"
  type: "Convolution"
  bottom: "172"
  top: "173"
  convolution_param {
    num_output: 32
    bias_term: true
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "LeakyRelu_7"
  type: "ReLU"
  bottom: "173"
  top: "174"
  relu_param {
    negative_slope: 0.10000000149011612
  }
}
layer {
  name: "Conv_8"
  type: "Convolution"
  bottom: "174"
  top: "175"
  convolution_param {
    num_output: 32
    bias_term: true
    group: 1
    pad_h: 1
    pad_w: 1
    kernel_h: 3
    kernel_w: 3
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "LeakyRelu_9"
  type: "ReLU"
  bottom: "175"
  top: "176"
  relu_param {
    negative_slope: 0.10000000149011612
  }
}
layer {
  name: "Add_10"
  type: "Eltwise"
  bottom: "172"
  bottom: "176"
  top: "177"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "Conv_11"
  type: "Convolution"
  bottom: "177"
  top: "178"
  convolution_param {
    num_output: 32
    bias_term: false
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "Conv_12"
  type: "Convolution"
  bottom: "170"
  top: "179"
  convolution_param {
    num_output: 32
    bias_term: false
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "Concat_13"
  type: "Concat"
  bottom: "178"
  bottom: "179"
  top: "180"
  concat_param {
    axis: 1
  }
}
layer {
  name: "BatchNormalization_14_bn"
  type: "BatchNorm"
  bottom: "180"
  top: "181"
  batch_norm_param {
    use_global_stats: true
    eps: 0.0010000000474974513
  }
}
layer {
  name: "BatchNormalization_14"
  type: "Scale"
  bottom: "181"
  top: "181"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "LeakyRelu_15"
  type: "ReLU"
  bottom: "181"
  top: "182"
  relu_param {
    negative_slope: 0.10000000149011612
  }
}
layer {
  name: "Conv_16"
  type: "Convolution"
  bottom: "182"
  top: "183"
  convolution_param {
    num_output: 64
    bias_term: true
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "LeakyRelu_17"
  type: "ReLU"
  bottom: "183"
  top: "184"
  relu_param {
    negative_slope: 0.10000000149011612
  }
}
layer {
  name: "Conv_18"
  type: "Convolution"
  bottom: "184"
  top: "185"
  convolution_param {
    num_output: 128
    bias_term: true
    group: 1
    pad_h: 1
    pad_w: 1
    kernel_h: 3
    kernel_w: 3
    stride_h: 2
    stride_w: 2
    dilation: 1
  }
}
layer {
  name: "LeakyRelu_19"
  type: "ReLU"
  bottom: "185"
  top: "186"
  relu_param {
    negative_slope: 0.10000000149011612
  }
}
layer {
  name: "Conv_20"
  type: "Convolution"
  bottom: "186"
  top: "187"
  convolution_param {
    num_output: 64
    bias_term: true
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "LeakyRelu_21"
  type: "ReLU"
  bottom: "187"
  top: "188"
  relu_param {
    negative_slope: 0.10000000149011612
  }
}
layer {
  name: "Conv_22"
  type: "Convolution"
  bottom: "188"
  top: "189"
  convolution_param {
    num_output: 64
    bias_term: true
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "LeakyRelu_23"
  type: "ReLU"
  bottom: "189"
  top: "190"
  relu_param {
    negative_slope: 0.10000000149011612
  }
}
layer {
  name: "Conv_24"
  type: "Convolution"
  bottom: "190"
  top: "191"
  convolution_param {
    num_output: 64
    bias_term: true
    group: 1
    pad_h: 1
    pad_w: 1
    kernel_h: 3
    kernel_w: 3
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "LeakyRelu_25"
  type: "ReLU"
  bottom: "191"
  top: "192"
  relu_param {
    negative_slope: 0.10000000149011612
  }
}
layer {
  name: "Add_26"
  type: "Eltwise"
  bottom: "188"
  bottom: "192"
  top: "193"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "Conv_27"
  type: "Convolution"
  bottom: "193"
  top: "194"
  convolution_param {
    num_output: 64
    bias_term: true
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "LeakyRelu_28"
  type: "ReLU"
  bottom: "194"
  top: "195"
  relu_param {
    negative_slope: 0.10000000149011612
  }
}
layer {
  name: "Conv_29"
  type: "Convolution"
  bottom: "195"
  top: "196"
  convolution_param {
    num_output: 64
    bias_term: true
    group: 1
    pad_h: 1
    pad_w: 1
    kernel_h: 3
    kernel_w: 3
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "LeakyRelu_30"
  type: "ReLU"
  bottom: "196"
  top: "197"
  relu_param {
    negative_slope: 0.10000000149011612
  }
}
layer {
  name: "Add_31"
  type: "Eltwise"
  bottom: "193"
  bottom: "197"
  top: "198"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "Conv_32"
  type: "Convolution"
  bottom: "198"
  top: "199"
  convolution_param {
    num_output: 64
    bias_term: true
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "LeakyRelu_33"
  type: "ReLU"
  bottom: "199"
  top: "200"
  relu_param {
    negative_slope: 0.10000000149011612
  }
}
layer {
  name: "Conv_34"
  type: "Convolution"
  bottom: "200"
  top: "201"
  convolution_param {
    num_output: 64
    bias_term: true
    group: 1
    pad_h: 1
    pad_w: 1
    kernel_h: 3
    kernel_w: 3
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "LeakyRelu_35"
  type: "ReLU"
  bottom: "201"
  top: "202"
  relu_param {
    negative_slope: 0.10000000149011612
  }
}
layer {
  name: "Add_36"
  type: "Eltwise"
  bottom: "198"
  bottom: "202"
  top: "203"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "Conv_37"
  type: "Convolution"
  bottom: "203"
  top: "204"
  convolution_param {
    num_output: 64
    bias_term: false
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "Conv_38"
  type: "Convolution"
  bottom: "186"
  top: "205"
  convolution_param {
    num_output: 64
    bias_term: false
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "Concat_39"
  type: "Concat"
  bottom: "204"
  bottom: "205"
  top: "206"
  concat_param {
    axis: 1
  }
}
layer {
  name: "BatchNormalization_40_bn"
  type: "BatchNorm"
  bottom: "206"
  top: "207"
  batch_norm_param {
    use_global_stats: true
    eps: 0.0010000000474974513
  }
}
layer {
  name: "BatchNormalization_40"
  type: "Scale"
  bottom: "207"
  top: "207"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "LeakyRelu_41"
  type: "ReLU"
  bottom: "207"
  top: "208"
  relu_param {
    negative_slope: 0.10000000149011612
  }
}
layer {
  name: "Conv_42"
  type: "Convolution"
  bottom: "208"
  top: "209"
  convolution_param {
    num_output: 128
    bias_term: true
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "LeakyRelu_43"
  type: "ReLU"
  bottom: "209"
  top: "210"
  relu_param {
    negative_slope: 0.10000000149011612
  }
}
layer {
  name: "Conv_44"
  type: "Convolution"
  bottom: "210"
  top: "211"
  convolution_param {
    num_output: 256
    bias_term: true
    group: 1
    pad_h: 1
    pad_w: 1
    kernel_h: 3
    kernel_w: 3
    stride_h: 2
    stride_w: 2
    dilation: 1
  }
}
layer {
  name: "LeakyRelu_45"
  type: "ReLU"
  bottom: "211"
  top: "212"
  relu_param {
    negative_slope: 0.10000000149011612
  }
}
layer {
  name: "Conv_46"
  type: "Convolution"
  bottom: "212"
  top: "213"
  convolution_param {
    num_output: 128
    bias_term: true
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "LeakyRelu_47"
  type: "ReLU"
  bottom: "213"
  top: "214"
  relu_param {
    negative_slope: 0.10000000149011612
  }
}
layer {
  name: "Conv_48"
  type: "Convolution"
  bottom: "214"
  top: "215"
  convolution_param {
    num_output: 128
    bias_term: true
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "LeakyRelu_49"
  type: "ReLU"
  bottom: "215"
  top: "216"
  relu_param {
    negative_slope: 0.10000000149011612
  }
}
layer {
  name: "Conv_50"
  type: "Convolution"
  bottom: "216"
  top: "217"
  convolution_param {
    num_output: 128
    bias_term: true
    group: 1
    pad_h: 1
    pad_w: 1
    kernel_h: 3
    kernel_w: 3
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "LeakyRelu_51"
  type: "ReLU"
  bottom: "217"
  top: "218"
  relu_param {
    negative_slope: 0.10000000149011612
  }
}
layer {
  name: "Add_52"
  type: "Eltwise"
  bottom: "214"
  bottom: "218"
  top: "219"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "Conv_53"
  type: "Convolution"
  bottom: "219"
  top: "220"
  convolution_param {
    num_output: 128
    bias_term: true
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "LeakyRelu_54"
  type: "ReLU"
  bottom: "220"
  top: "221"
  relu_param {
    negative_slope: 0.10000000149011612
  }
}
layer {
  name: "Conv_55"
  type: "Convolution"
  bottom: "221"
  top: "222"
  convolution_param {
    num_output: 128
    bias_term: true
    group: 1
    pad_h: 1
    pad_w: 1
    kernel_h: 3
    kernel_w: 3
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "LeakyRelu_56"
  type: "ReLU"
  bottom: "222"
  top: "223"
  relu_param {
    negative_slope: 0.10000000149011612
  }
}
layer {
  name: "Add_57"
  type: "Eltwise"
  bottom: "219"
  bottom: "223"
  top: "224"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "Conv_58"
  type: "Convolution"
  bottom: "224"
  top: "225"
  convolution_param {
    num_output: 128
    bias_term: true
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "LeakyRelu_59"
  type: "ReLU"
  bottom: "225"
  top: "226"
  relu_param {
    negative_slope: 0.10000000149011612
  }
}
layer {
  name: "Conv_60"
  type: "Convolution"
  bottom: "226"
  top: "227"
  convolution_param {
    num_output: 128
    bias_term: true
    group: 1
    pad_h: 1
    pad_w: 1
    kernel_h: 3
    kernel_w: 3
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "LeakyRelu_61"
  type: "ReLU"
  bottom: "227"
  top: "228"
  relu_param {
    negative_slope: 0.10000000149011612
  }
}
layer {
  name: "Add_62"
  type: "Eltwise"
  bottom: "224"
  bottom: "228"
  top: "229"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "Conv_63"
  type: "Convolution"
  bottom: "229"
  top: "230"
  convolution_param {
    num_output: 128
    bias_term: false
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "Conv_64"
  type: "Convolution"
  bottom: "212"
  top: "231"
  convolution_param {
    num_output: 128
    bias_term: false
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "Concat_65"
  type: "Concat"
  bottom: "230"
  bottom: "231"
  top: "232"
  concat_param {
    axis: 1
  }
}
layer {
  name: "BatchNormalization_66_bn"
  type: "BatchNorm"
  bottom: "232"
  top: "233"
  batch_norm_param {
    use_global_stats: true
    eps: 0.0010000000474974513
  }
}
layer {
  name: "BatchNormalization_66"
  type: "Scale"
  bottom: "233"
  top: "233"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "LeakyRelu_67"
  type: "ReLU"
  bottom: "233"
  top: "234"
  relu_param {
    negative_slope: 0.10000000149011612
  }
}
layer {
  name: "Conv_68"
  type: "Convolution"
  bottom: "234"
  top: "235"
  convolution_param {
    num_output: 256
    bias_term: true
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "LeakyRelu_69"
  type: "ReLU"
  bottom: "235"
  top: "236"
  relu_param {
    negative_slope: 0.10000000149011612
  }
}
layer {
  name: "Conv_70"
  type: "Convolution"
  bottom: "236"
  top: "237"
  convolution_param {
    num_output: 512
    bias_term: true
    group: 1
    pad_h: 1
    pad_w: 1
    kernel_h: 3
    kernel_w: 3
    stride_h: 2
    stride_w: 2
    dilation: 1
  }
}
layer {
  name: "LeakyRelu_71"
  type: "ReLU"
  bottom: "237"
  top: "238"
  relu_param {
    negative_slope: 0.10000000149011612
  }
}
layer {
  name: "Conv_72"
  type: "Convolution"
  bottom: "238"
  top: "239"
  convolution_param {
    num_output: 256
    bias_term: true
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "LeakyRelu_73"
  type: "ReLU"
  bottom: "239"
  top: "240"
  relu_param {
    negative_slope: 0.10000000149011612
  }
}
layer {
  name: "MaxPool_74"
  type: "Pooling"
  bottom: "240"
  top: "241"
  pooling_param {
    pool: MAX
    kernel_h: 5
    kernel_w: 5
    stride_h: 1
    stride_w: 1
    pad_h: 2
    pad_w: 2
  }
}
layer {
  name: "MaxPool_75"
  type: "Pooling"
  bottom: "240"
  top: "242"
  pooling_param {
    pool: MAX
    kernel_h: 9
    kernel_w: 9
    stride_h: 1
    stride_w: 1
    pad_h: 4
    pad_w: 4
  }
}
layer {
  name: "MaxPool_76"
  type: "Pooling"
  bottom: "240"
  top: "243"
  pooling_param {
    pool: MAX
    kernel_h: 13
    kernel_w: 13
    stride_h: 1
    stride_w: 1
    pad_h: 6
    pad_w: 6
  }
}
layer {
  name: "Concat_77"
  type: "Concat"
  bottom: "240"
  bottom: "241"
  bottom: "242"
  bottom: "243"
  top: "244"
  concat_param {
    axis: 1
  }
}
layer {
  name: "Conv_78"
  type: "Convolution"
  bottom: "244"
  top: "245"
  convolution_param {
    num_output: 512
    bias_term: true
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "LeakyRelu_79"
  type: "ReLU"
  bottom: "245"
  top: "246"
  relu_param {
    negative_slope: 0.10000000149011612
  }
}
layer {
  name: "Conv_80"
  type: "Convolution"
  bottom: "246"
  top: "247"
  convolution_param {
    num_output: 256
    bias_term: true
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "LeakyRelu_81"
  type: "ReLU"
  bottom: "247"
  top: "248"
  relu_param {
    negative_slope: 0.10000000149011612
  }
}
layer {
  name: "Conv_82"
  type: "Convolution"
  bottom: "248"
  top: "249"
  convolution_param {
    num_output: 256
    bias_term: true
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "LeakyRelu_83"
  type: "ReLU"
  bottom: "249"
  top: "250"
  relu_param {
    negative_slope: 0.10000000149011612
  }
}
layer {
  name: "Conv_84"
  type: "Convolution"
  bottom: "250"
  top: "251"
  convolution_param {
    num_output: 256
    bias_term: true
    group: 1
    pad_h: 1
    pad_w: 1
    kernel_h: 3
    kernel_w: 3
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "LeakyRelu_85"
  type: "ReLU"
  bottom: "251"
  top: "252"
  relu_param {
    negative_slope: 0.10000000149011612
  }
}
layer {
  name: "Conv_86"
  type: "Convolution"
  bottom: "252"
  top: "253"
  convolution_param {
    num_output: 256
    bias_term: false
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "Conv_87"
  type: "Convolution"
  bottom: "246"
  top: "254"
  convolution_param {
    num_output: 256
    bias_term: false
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "Concat_88"
  type: "Concat"
  bottom: "253"
  bottom: "254"
  top: "255"
  concat_param {
    axis: 1
  }
}
layer {
  name: "BatchNormalization_89_bn"
  type: "BatchNorm"
  bottom: "255"
  top: "256"
  batch_norm_param {
    use_global_stats: true
    eps: 0.0010000000474974513
  }
}
layer {
  name: "BatchNormalization_89"
  type: "Scale"
  bottom: "256"
  top: "256"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "LeakyRelu_90"
  type: "ReLU"
  bottom: "256"
  top: "257"
  relu_param {
    negative_slope: 0.10000000149011612
  }
}
layer {
  name: "Conv_91"
  type: "Convolution"
  bottom: "257"
  top: "258"
  convolution_param {
    num_output: 512
    bias_term: true
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "LeakyRelu_92"
  type: "ReLU"
  bottom: "258"
  top: "259"
  relu_param {
    negative_slope: 0.10000000149011612
  }
}
layer {
  name: "Conv_93"
  type: "Convolution"
  bottom: "259"
  top: "260"
  convolution_param {
    num_output: 256
    bias_term: true
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "LeakyRelu_94"
  type: "ReLU"
  bottom: "260"
  top: "261"
  relu_param {
    negative_slope: 0.10000000149011612
  }
}
layer {
  name: "Upsample_95"
  type: "Upsample"
  bottom: "261"
  top: "265"
  upsample_param {
    height_scale: 2
    width_scale: 2
    mode: NEAREST
  }
}
layer {
  name: "Concat_96"
  type: "Concat"
  bottom: "265"
  bottom: "236"
  top: "266"
  concat_param {
    axis: 1
  }
}
layer {
  name: "Conv_97"
  type: "Convolution"
  bottom: "266"
  top: "267"
  convolution_param {
    num_output: 128
    bias_term: true
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "LeakyRelu_98"
  type: "ReLU"
  bottom: "267"
  top: "268"
  relu_param {
    negative_slope: 0.10000000149011612
  }
}
layer {
  name: "Conv_99"
  type: "Convolution"
  bottom: "268"
  top: "269"
  convolution_param {
    num_output: 128
    bias_term: true
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "LeakyRelu_100"
  type: "ReLU"
  bottom: "269"
  top: "270"
  relu_param {
    negative_slope: 0.10000000149011612
  }
}
layer {
  name: "Conv_101"
  type: "Convolution"
  bottom: "270"
  top: "271"
  convolution_param {
    num_output: 128
    bias_term: true
    group: 1
    pad_h: 1
    pad_w: 1
    kernel_h: 3
    kernel_w: 3
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "LeakyRelu_102"
  type: "ReLU"
  bottom: "271"
  top: "272"
  relu_param {
    negative_slope: 0.10000000149011612
  }
}
layer {
  name: "Conv_103"
  type: "Convolution"
  bottom: "272"
  top: "273"
  convolution_param {
    num_output: 128
    bias_term: false
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "Conv_104"
  type: "Convolution"
  bottom: "266"
  top: "274"
  convolution_param {
    num_output: 128
    bias_term: false
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "Concat_105"
  type: "Concat"
  bottom: "273"
  bottom: "274"
  top: "275"
  concat_param {
    axis: 1
  }
}
layer {
  name: "BatchNormalization_106_bn"
  type: "BatchNorm"
  bottom: "275"
  top: "276"
  batch_norm_param {
    use_global_stats: true
    eps: 0.0010000000474974513
  }
}
layer {
  name: "BatchNormalization_106"
  type: "Scale"
  bottom: "276"
  top: "276"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "LeakyRelu_107"
  type: "ReLU"
  bottom: "276"
  top: "277"
  relu_param {
    negative_slope: 0.10000000149011612
  }
}
layer {
  name: "Conv_108"
  type: "Convolution"
  bottom: "277"
  top: "278"
  convolution_param {
    num_output: 256
    bias_term: true
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "LeakyRelu_109"
  type: "ReLU"
  bottom: "278"
  top: "279"
  relu_param {
    negative_slope: 0.10000000149011612
  }
}
layer {
  name: "Conv_110"
  type: "Convolution"
  bottom: "279"
  top: "280"
  convolution_param {
    num_output: 128
    bias_term: true
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "LeakyRelu_111"
  type: "ReLU"
  bottom: "280"
  top: "281"
  relu_param {
    negative_slope: 0.10000000149011612
  }
}
layer {
  name: "Upsample_112"
  type: "Upsample"
  bottom: "281"
  top: "285"
  upsample_param {
    height_scale: 2
    width_scale: 2
    mode: NEAREST
  }
}
layer {
  name: "Concat_113"
  type: "Concat"
  bottom: "285"
  bottom: "210"
  top: "286"
  concat_param {
    axis: 1
  }
}
layer {
  name: "Conv_114"
  type: "Convolution"
  bottom: "286"
  top: "287"
  convolution_param {
    num_output: 64
    bias_term: true
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "LeakyRelu_115"
  type: "ReLU"
  bottom: "287"
  top: "288"
  relu_param {
    negative_slope: 0.10000000149011612
  }
}
layer {
  name: "Conv_116"
  type: "Convolution"
  bottom: "288"
  top: "289"
  convolution_param {
    num_output: 64
    bias_term: true
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "LeakyRelu_117"
  type: "ReLU"
  bottom: "289"
  top: "290"
  relu_param {
    negative_slope: 0.10000000149011612
  }
}
layer {
  name: "Conv_118"
  type: "Convolution"
  bottom: "290"
  top: "291"
  convolution_param {
    num_output: 64
    bias_term: true
    group: 1
    pad_h: 1
    pad_w: 1
    kernel_h: 3
    kernel_w: 3
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "LeakyRelu_119"
  type: "ReLU"
  bottom: "291"
  top: "292"
  relu_param {
    negative_slope: 0.10000000149011612
  }
}
layer {
  name: "Conv_120"
  type: "Convolution"
  bottom: "292"
  top: "293"
  convolution_param {
    num_output: 64
    bias_term: false
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "Conv_121"
  type: "Convolution"
  bottom: "286"
  top: "294"
  convolution_param {
    num_output: 64
    bias_term: false
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "Concat_122"
  type: "Concat"
  bottom: "293"
  bottom: "294"
  top: "295"
  concat_param {
    axis: 1
  }
}
layer {
  name: "BatchNormalization_123_bn"
  type: "BatchNorm"
  bottom: "295"
  top: "296"
  batch_norm_param {
    use_global_stats: true
    eps: 0.0010000000474974513
  }
}
layer {
  name: "BatchNormalization_123"
  type: "Scale"
  bottom: "296"
  top: "296"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "LeakyRelu_124"
  type: "ReLU"
  bottom: "296"
  top: "297"
  relu_param {
    negative_slope: 0.10000000149011612
  }
}
layer {
  name: "Conv_125"
  type: "Convolution"
  bottom: "297"
  top: "298"
  convolution_param {
    num_output: 128
    bias_term: true
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "LeakyRelu_126"
  type: "ReLU"
  bottom: "298"
  top: "299"
  relu_param {
    negative_slope: 0.10000000149011612
  }
}
layer {
  name: "Conv_127"
  type: "Convolution"
  bottom: "299"
  top: "300"
  convolution_param {
    num_output: 128
    bias_term: true
    group: 1
    pad_h: 1
    pad_w: 1
    kernel_h: 3
    kernel_w: 3
    stride_h: 2
    stride_w: 2
    dilation: 1
  }
}
layer {
  name: "LeakyRelu_128"
  type: "ReLU"
  bottom: "300"
  top: "301"
  relu_param {
    negative_slope: 0.10000000149011612
  }
}
layer {
  name: "Concat_129"
  type: "Concat"
  bottom: "301"
  bottom: "281"
  top: "302"
  concat_param {
    axis: 1
  }
}
layer {
  name: "Conv_130"
  type: "Convolution"
  bottom: "302"
  top: "303"
  convolution_param {
    num_output: 128
    bias_term: true
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "LeakyRelu_131"
  type: "ReLU"
  bottom: "303"
  top: "304"
  relu_param {
    negative_slope: 0.10000000149011612
  }
}
layer {
  name: "Conv_132"
  type: "Convolution"
  bottom: "304"
  top: "305"
  convolution_param {
    num_output: 128
    bias_term: true
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "LeakyRelu_133"
  type: "ReLU"
  bottom: "305"
  top: "306"
  relu_param {
    negative_slope: 0.10000000149011612
  }
}
layer {
  name: "Conv_134"
  type: "Convolution"
  bottom: "306"
  top: "307"
  convolution_param {
    num_output: 128
    bias_term: true
    group: 1
    pad_h: 1
    pad_w: 1
    kernel_h: 3
    kernel_w: 3
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "LeakyRelu_135"
  type: "ReLU"
  bottom: "307"
  top: "308"
  relu_param {
    negative_slope: 0.10000000149011612
  }
}
layer {
  name: "Conv_136"
  type: "Convolution"
  bottom: "308"
  top: "309"
  convolution_param {
    num_output: 128
    bias_term: false
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "Conv_137"
  type: "Convolution"
  bottom: "302"
  top: "310"
  convolution_param {
    num_output: 128
    bias_term: false
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "Concat_138"
  type: "Concat"
  bottom: "309"
  bottom: "310"
  top: "311"
  concat_param {
    axis: 1
  }
}
layer {
  name: "BatchNormalization_139_bn"
  type: "BatchNorm"
  bottom: "311"
  top: "312"
  batch_norm_param {
    use_global_stats: true
    eps: 0.0010000000474974513
  }
}
layer {
  name: "BatchNormalization_139"
  type: "Scale"
  bottom: "312"
  top: "312"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "LeakyRelu_140"
  type: "ReLU"
  bottom: "312"
  top: "313"
  relu_param {
    negative_slope: 0.10000000149011612
  }
}
layer {
  name: "Conv_141"
  type: "Convolution"
  bottom: "313"
  top: "314"
  convolution_param {
    num_output: 256
    bias_term: true
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "LeakyRelu_142"
  type: "ReLU"
  bottom: "314"
  top: "315"
  relu_param {
    negative_slope: 0.10000000149011612
  }
}
layer {
  name: "Conv_143"
  type: "Convolution"
  bottom: "315"
  top: "316"
  convolution_param {
    num_output: 256
    bias_term: true
    group: 1
    pad_h: 1
    pad_w: 1
    kernel_h: 3
    kernel_w: 3
    stride_h: 2
    stride_w: 2
    dilation: 1
  }
}
layer {
  name: "LeakyRelu_144"
  type: "ReLU"
  bottom: "316"
  top: "317"
  relu_param {
    negative_slope: 0.10000000149011612
  }
}
layer {
  name: "Concat_145"
  type: "Concat"
  bottom: "317"
  bottom: "261"
  top: "318"
  concat_param {
    axis: 1
  }
}
layer {
  name: "Conv_146"
  type: "Convolution"
  bottom: "318"
  top: "319"
  convolution_param {
    num_output: 256
    bias_term: true
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "LeakyRelu_147"
  type: "ReLU"
  bottom: "319"
  top: "320"
  relu_param {
    negative_slope: 0.10000000149011612
  }
}
layer {
  name: "Conv_148"
  type: "Convolution"
  bottom: "320"
  top: "321"
  convolution_param {
    num_output: 256
    bias_term: true
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "LeakyRelu_149"
  type: "ReLU"
  bottom: "321"
  top: "322"
  relu_param {
    negative_slope: 0.10000000149011612
  }
}
layer {
  name: "Conv_150"
  type: "Convolution"
  bottom: "322"
  top: "323"
  convolution_param {
    num_output: 256
    bias_term: true
    group: 1
    pad_h: 1
    pad_w: 1
    kernel_h: 3
    kernel_w: 3
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "LeakyRelu_151"
  type: "ReLU"
  bottom: "323"
  top: "324"
  relu_param {
    negative_slope: 0.10000000149011612
  }
}
layer {
  name: "Conv_152"
  type: "Convolution"
  bottom: "324"
  top: "325"
  convolution_param {
    num_output: 256
    bias_term: false
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "Conv_153"
  type: "Convolution"
  bottom: "318"
  top: "326"
  convolution_param {
    num_output: 256
    bias_term: false
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "Concat_154"
  type: "Concat"
  bottom: "325"
  bottom: "326"
  top: "327"
  concat_param {
    axis: 1
  }
}
layer {
  name: "BatchNormalization_155_bn"
  type: "BatchNorm"
  bottom: "327"
  top: "328"
  batch_norm_param {
    use_global_stats: true
    eps: 0.0010000000474974513
  }
}
layer {
  name: "BatchNormalization_155"
  type: "Scale"
  bottom: "328"
  top: "328"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "LeakyRelu_156"
  type: "ReLU"
  bottom: "328"
  top: "329"
  relu_param {
    negative_slope: 0.10000000149011612
  }
}
layer {
  name: "Conv_157"
  type: "Convolution"
  bottom: "329"
  top: "330"
  convolution_param {
    num_output: 512
    bias_term: true
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "LeakyRelu_158"
  type: "ReLU"
  bottom: "330"
  top: "331"
  relu_param {
    negative_slope: 0.10000000149011612
  }
}
layer {
  name: "Conv_159"
  type: "Convolution"
  bottom: "299"
  top: "332"
  convolution_param {
    num_output: 18
    bias_term: true
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "Reshape_173"
  type: "Reshape"
  bottom: "332"
  top: "350"
  reshape_param {
    shape {
      dim: 1
      dim: 3
      dim: 6
      dim: 80
      dim: 80
    }
  }
}
layer {
  name: "Transpose_174"
  type: "Permute"
  bottom: "350"
  top: "output"
  permute_param {
    order: 0
    order: 1
    order: 3
    order: 4
    order: 2
  }
}
layer {
  name: "Conv_175"
  type: "Convolution"
  bottom: "315"
  top: "352"
  convolution_param {
    num_output: 18
    bias_term: true
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "Reshape_189"
  type: "Reshape"
  bottom: "352"
  top: "370"
  reshape_param {
    shape {
      dim: 1
      dim: 3
      dim: 6
      dim: 40
      dim: 40
    }
  }
}
layer {
  name: "Transpose_190"
  type: "Permute"
  bottom: "370"
  top: "371"
  permute_param {
    order: 0
    order: 1
    order: 3
    order: 4
    order: 2
  }
}
layer {
  name: "Conv_191"
  type: "Convolution"
  bottom: "331"
  top: "372"
  convolution_param {
    num_output: 18
    bias_term: true
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "Reshape_205"
  type: "Reshape"
  bottom: "372"
  top: "390"
  reshape_param {
    shape {
      dim: 1
      dim: 3
      dim: 6
      dim: 20
      dim: 20
    }
  }
}
layer {
  name: "Transpose_206"
  type: "Permute"
  bottom: "390"
  top: "391"
  permute_param {
    order: 0
    order: 1
    order: 3
    order: 4
    order: 2
  }
}

